# HÆ°á»›ng Dáº«n CÃ i Äáº·t Softmax Regression Tá»« Zero Báº±ng Python

Trong bÃ i viáº¿t nÃ y, chÃºng ta sáº½ tá»± xÃ¢y dá»±ng **Softmax Regression** â€” mÃ´ hÃ¬nh phÃ¢n loáº¡i Ä‘a lá»›p â€” hoÃ n toÃ n báº±ng **NumPy**.

Má»i khÃ¡i niá»‡m Ä‘á»u dá»±a trÃªn tÃ i liá»‡u **Softmax Regression** báº¡n Ä‘Ã£ táº£i lÃªn.

BÃ i viáº¿t sáº½ giÃºp báº¡n:

- Hiá»ƒu *one-hot encoding*  
- Tá»± xÃ¢y *Softmax function*  
- TÃ­nh *cross-entropy loss*  
- TÃ­nh *gradient* báº±ng cÃ´ng thá»©c tá»« tÃ i liá»‡u  
- Cáº­p nháº­t tham sá»‘ báº±ng *gradient descent*  


![Screenshot 2025-11-24 203640.png](/static/uploads/20251124_203704_7ceff515.png)

# Giai Ä‘oáº¡n Huáº¥n luyá»‡n (Training Phase)

1. **Khá»Ÿi táº¡o trá»ng sá»‘ (Initialize weights)**  
   Báº¯t Ä‘áº§u báº±ng cÃ¡ch gÃ¡n giÃ¡ trá»‹ ban Ä‘áº§u cho cÃ¡c trá»ng sá»‘ vÃ  bias. ThÆ°á»ng dÃ¹ng giÃ¡ trá»‹ ngáº«u nhiÃªn hoáº·c theo má»™t phÆ°Æ¡ng phÃ¡p chuáº©n hÃ³a.

2. **Chá»n má»™t máº«u (x, y) tá»« dá»¯ liá»‡u huáº¥n luyá»‡n (Pick a sample)**  
   Láº¥y tá»«ng cáº·p dá»¯ liá»‡u Ä‘áº§u vÃ o vÃ  nhÃ£n tÆ°Æ¡ng á»©ng Ä‘á»ƒ tÃ­nh toÃ¡n.

3. **TÃ­nh Ä‘áº§u ra dá»± Ä‘oÃ¡n $( \hat{y} )$ (Compute output)**  
   DÃ¹ng trá»ng sá»‘ hiá»‡n táº¡i Ä‘á»ƒ dá»± Ä‘oÃ¡n Ä‘áº§u ra tá»« máº«u dá»¯ liá»‡u. ÄÃ¢y lÃ  bÆ°á»›c forward propagation.

4. **TÃ­nh loss (Compute loss)**  
   So sÃ¡nh giÃ¡ trá»‹ dá»± Ä‘oÃ¡n $( \hat{y} )$ vá»›i giÃ¡ trá»‹ tháº­t y Ä‘á»ƒ tÃ­nh hÃ m máº¥t mÃ¡t (loss).

5. **TÃ­nh Ä‘áº¡o hÃ m (Compute derivative)**  
   TÃ­nh gradient cá»§a loss theo cÃ¡c trá»ng sá»‘. ÄÃ¢y lÃ  bÆ°á»›c quan trá»ng Ä‘á»ƒ biáº¿t cáº§n Ä‘iá»u chá»‰nh trá»ng sá»‘ nhÆ° tháº¿ nÃ o.

6. **Cáº­p nháº­t tham sá»‘ (Update parameters)**  
   Sá»­ dá»¥ng gradient vÃ  learning rate Ä‘á»ƒ cáº­p nháº­t trá»ng sá»‘, nháº±m giáº£m loss.

7. **Láº·p láº¡i tá»« bÆ°á»›c 2 cho máº«u tiáº¿p theo (Repeat from step 2)**  
   Tiáº¿p tá»¥c vá»›i cÃ¡c máº«u khÃ¡c cho Ä‘áº¿n khi toÃ n bá»™ dá»¯ liá»‡u Ä‘Æ°á»£c huáº¥n luyá»‡n hoáº·c Ä‘áº¡t Ä‘iá»u kiá»‡n dá»«ng.

ğŸ’¡ **ChÃº thÃ­ch thÃªm:** QuÃ¡ trÃ¬nh nÃ y láº·p Ä‘i láº·p láº¡i nhiá»u láº§n (epochs) trÃªn toÃ n bá»™ táº­p dá»¯ liá»‡u Ä‘á»ƒ mÃ´ hÃ¬nh há»c Ä‘Æ°á»£c cÃ¡c Ä‘áº·c trÆ°ng vÃ  giáº£m lá»—i dá»± Ä‘oÃ¡n.


## B1: Chuáº©n bá»‹ dá»¯ liá»‡u vÃ  One-hot Encoding

TÃ i liá»‡u chá»‰ ra ráº±ng cross-entropy nhiá»u lá»›p Ä‘Æ°á»£c viáº¿t gá»n báº±ng one-hot:

$$
L = -\sum_{j=1}^{k} y_j \log(\hat{y}_j)
$$

NÃªn ta cáº§n chuyá»ƒn nhÃ£n (0, 1, â€¦, kâˆ’1) â†’ vector one-hot.

###### 1.1 Code one-hot encoding:

```python
def convert_one_hot(y, k):  
    one_hot = np.zeros((len(y), k))
    one_hot[np.arange(len(y)), y] = 1
    return one_hot
```

###### 1.2 ThÃªm cá»™t Intercept

Softmax Regression dÃ¹ng:

$$
z = \theta^T x = 
\begin{bmatrix}
b \\
w
\end{bmatrix}^T
\begin{bmatrix}
1 \\
x
\end{bmatrix}
$$

NÃªn ta thÃªm 1 cá»™t toÃ n sá»‘ 1 vÃ o ma tráº­n `X`:

```python
  intercept = np.ones((X.shape[0], 1))
  X = np.concatenate((intercept, X), axis=1)
```

### B2: Khá»Ÿi táº¡o tham sá»‘ Î¸

Náº¿u sá»‘ chiá»u cá»§a input = 1 â†’ Î¸ cÃ³ shape (2 Ã— k).

VÃ­ dá»¥:

```python
theta = np.array([[0.1, 0.05], 
                  [0.2, -0.1]])
```

### B3: VÃ²ng láº·p huáº¥n luyá»‡n Softmax Regression

Huáº¥n luyá»‡n dá»±a theo cÃ´ng thá»©c gradient trong tÃ i liá»‡u:

**Forward:**

$$
z = \theta^T x
$$

$$
\hat{y} = \text{softmax}(z)
$$

**Loss (cross entropy):**

$$
L = -y^T \log(\hat{y})
$$

**Gradient:**

$$
\frac{\partial L}{\partial \theta} = x(\hat{y} - y)^T
$$

ğŸ”¥ **ToÃ n bá»™ vÃ²ng láº·p training:**

```python
learning_rate = 0.1
losses = []
max_epoch = 1

for epoch in range(max_epoch):
    for i in range(N): 
        xi = X[i]
        yi = y_one_hot[i]
        
        # reshape to column vectors
        xi = xi.reshape((2,1))
        yi = yi.reshape((2,1))
        
        # compute z
        z = theta.T.dot(xi)        
        
        # compute y_hat (softmax)
        exp_z = np.exp(z)
        y_hat = exp_z / np.sum(exp_z, axis=0)
        
        # compute loss
        loss = -yi.T.dot(np.log(y_hat))
        losses.append(loss[0])
        
        # compute gradient
        dz = y_hat - yi              # (2Ã—1)
        dtheta = xi.dot(dz.T)        # (2Ã—2)
        
        # update parameters
        theta = theta - learning_rate * dtheta
```


### Giáº£i thÃ­ch tá»«ng bÆ°á»›c

âœ” **Forward pass**  
Ta tÃ­nh $ z = \theta^T x $ vÃ  softmax $(\hat{y})$.

âœ” **Loss**  
Dá»±a Ä‘Ãºng cÃ´ng thá»©c trong tÃ i liá»‡u:

$$
L = -y^T \log(\hat{y})
$$

VÃ¬ \(y\) lÃ  one-hot â†’ chá»‰ láº¥y log(p) cá»§a class Ä‘Ãºng.

âœ” **Gradient**  
TÃ i liá»‡u chá»©ng minh:

$$
\frac{\partial L}{\partial z} = \hat{y} - y
$$

Tá»« chain rule:

$$
\frac{\partial L}{\partial \theta} = x (\hat{y} - y)^T
$$

âœ” **Update**  

$$
\theta := \theta - \eta \frac{\partial L}{\partial \theta}
$$





