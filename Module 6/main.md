Giá»›i thiá»‡u vá» Softmax Regression vÃ  cÃ¡c hÃ m loss cho bÃ i toÃ¡n phÃ¢n loáº¡i Ä‘a nhÃ£n

# I. Váº¥n Ä‘á» cá»§a bÃ i toÃ¡n

Trong cÃ¡c bÃ i toÃ¡n phÃ¢n loáº¡i nhá»‹ phÃ¢n kinh Ä‘iá»ƒn, cháº³ng háº¡n nhÆ° phÃ¢n biá»‡t *spam / khÃ´ng spam* hay *bá»‡nh / khÃ´ng bá»‡nh*, mÃ´ hÃ¬nh **logistic** tá» ra Ä‘áº·c biá»‡t hiá»‡u quáº£. HÃ m *sigmoid* nháº­n má»™t giÃ¡ trá»‹ logit vÃ  biáº¿n nÃ³ thÃ nh xÃ¡c suáº¥t thuá»™c lá»›p â€œ1â€, trong khi xÃ¡c suáº¥t thuá»™c lá»›p â€œ0â€ chá»‰ Ä‘Æ¡n giáº£n lÃ  pháº§n bÃ¹ cÃ²n láº¡i. CÃ¡ch biá»ƒu diá»…n nÃ y vá»«a trá»±c quan, vá»«a phÃ¹ há»£p vá»›i trá»±c giÃ¡c xÃ¡c suáº¥t, nÃªn logistic nhanh chÃ³ng trá»Ÿ thÃ nh lá»±a chá»n máº·c Ä‘á»‹nh cho ráº¥t nhiá»u bÃ i toÃ¡n thá»±c táº¿ khi chá»‰ cÃ³ hai lá»›p cáº§n phÃ¢n biá»‡t.


Tuy nhiÃªn, khi chuyá»ƒn sang cÃ¡c bÃ i toÃ¡n phÃ¢n loáº¡i nhiá»u lá»›p â€“ cháº³ng háº¡n mÃ´ hÃ¬nh cáº§n phÃ¢n biá»‡t giá»¯a *mÃ¨o, chÃ³, chim, cÃ¡* â€“ viá»‡c â€œcá»‘ gáº¯ngâ€ sá»­ dá»¥ng láº¡i logistic theo cÃ¡ch truyá»n thá»‘ng báº¯t Ä‘áº§u bá»™c lá»™ nhiá»u háº¡n cháº¿. Má»™t phÆ°Æ¡ng phÃ¡p thÆ°á»ng gáº·p lÃ  huáº¥n luyá»‡n nhiá»u bá»™ phÃ¢n loáº¡i nhá»‹ phÃ¢n theo chiáº¿n lÆ°á»£c *one-vs-rest*, trong Ä‘Ã³ má»—i mÃ´ hÃ¬nh Ä‘áº£m nháº­n nhiá»‡m vá»¥ dá»± Ä‘oÃ¡n â€œmÃ¨o hay khÃ´ng mÃ¨oâ€, â€œchÃ³ hay khÃ´ng chÃ³â€,â€¦ Tá»« Ä‘Ã³, má»—i lá»›p táº¡o ra má»™t giÃ¡ trá»‹ xÃ¡c suáº¥t riÃªng biá»‡t. Váº¥n Ä‘á» náº±m á»Ÿ chá»— cÃ¡c xÃ¡c suáº¥t nÃ y **khÃ´ng cÃ³ báº¥t ká»³ rÃ ng buá»™c nÃ o vá»›i nhau**: tá»•ng cÃ³ thá»ƒ lá»›n hÆ¡n 1, nhiá»u lá»›p cÃ³ thá»ƒ cÃ¹ng nháº­n giÃ¡ trá»‹ ráº¥t cao, vÃ  mÃ´ hÃ¬nh khÃ´ng tháº­t sá»± bá»‹ buá»™c pháº£i lá»±a chá»n má»™t lá»›p ná»•i trá»™i nháº¥t. Äiá»u nÃ y khiáº¿n viá»‡c diá»…n giáº£i káº¿t quáº£ trá»Ÿ nÃªn thiáº¿u nháº¥t quÃ¡n: má»™t giÃ¡ trá»‹ xÃ¡c suáº¥t cao khÃ´ng cÃ²n Ä‘áº¡i diá»‡n rÃµ rÃ ng cho kháº£ nÄƒng â€œlá»›p nÃ y tá»‘t nháº¥tâ€ khi Ä‘áº·t trong bá»‘i cáº£nh cáº¡nh tranh giá»¯a toÃ n bá»™ cÃ¡c lá»›p. Káº¿t quáº£ cuá»‘i cÃ¹ng dá»… rÆ¡i vÃ o tÃ¬nh tráº¡ng mÆ¡ há»“ vÃ  thiáº¿u tÃ­nh xÃ¡c suáº¥t vá»›i bÃ i toÃ¡n Ä‘a lá»›p.

ChÃ­nh tá»« nhá»¯ng váº¥n Ä‘á» Ä‘Ã³ thÃ¬ cÃ³ má»™t cÆ¡ cháº¿ vá»«a tá»•ng quÃ¡t hÃ³a Ä‘Æ°á»£c logistic, vá»«a Ã©p toÃ n bá»™ dá»± Ä‘oÃ¡n náº±m trÃªn má»™t phÃ¢n phá»‘i xÃ¡c suáº¥t vá»›i sá»± cáº¡nh tranh rÃµ rÃ ng giá»¯a cÃ¡c lá»›p, hÃ m softmax Ä‘Æ°á»£c ra Ä‘á»i. Softmax cho phÃ©p biáº¿n toÃ n bá»™ vector logit thÃ nh má»™t phÃ¢n phá»‘i xÃ¡c suáº¥t chuáº©n hÃ³a trÃªn K lá»›p, qua Ä‘Ã³ kháº¯c phá»¥c cÃ¡c nhÆ°á»£c Ä‘iá»ƒm cá»§a cÃ¡c mÃ´ hÃ¬nh logistic trÆ°á»›c Ä‘Ã¢y vÃ  Ä‘áº·t ná»n táº£ng cho cÃ¡c kiáº¿n trÃºc phÃ¢n loáº¡i Ä‘a lá»›p hiá»‡n Ä‘áº¡i trong há»c mÃ¡y vÃ  deep learning.

# Há»“i quy Softmax (Softmax Regression)

## 2.1. Ã tÆ°á»Ÿng chÃ­nh
- Há»“i quy softmax (softmax regression) tá»•ng quÃ¡t hÃ³a mÃ´ hÃ¬nh há»“i quy logistic (logistic regression) Ä‘á»ƒ tá»‘i Æ°u hÃ³a lá»i giáº£i cho cÃ¡c bÃ i toÃ¡n phÃ¢n loáº¡i Ä‘a lá»›p. Thay vÃ¬ sá»­ dá»¥ng hÃ m sigmoid nhÆ° trong há»“i quy logistic, mÃ´ hÃ¬nh há»“i quy softmax biáº¿n cÃ¡c Ä‘iá»ƒm sá»‘ thÃ´ (hay cÃ²n gá»i lÃ  Ä‘iá»ƒm logit) thÃ nh má»™t phÃ¢n phá»‘i xÃ¡c suáº¥t rÃµ rÃ ng trÃªn táº¥t cáº£ cÃ¡c lá»›p. 

- *Váº­y cÃ¡c Ä‘iá»ƒm sá»‘ thÃ´ / Ä‘iá»ƒm logit nÃ y Ä‘áº¿n tá»« Ä‘Ã¢u?* NhÆ° trong há»“i quy logistic, Ä‘iá»ƒm logit lÃ  má»™t káº¿t quáº£ cá»§a 1 tá»• há»£p cÃ³ trá»ng sá»‘ cá»§a cÃ¡c Ä‘áº·c trÆ°ng Ä‘áº§u vÃ o vá»›i há»‡ sá»‘ nghiÃªng (bias). Má»—i logit trong há»“i quy softmax biá»ƒu diá»…n má»©c Ä‘á»™ áº£nh hÆ°á»Ÿng cá»§a cÃ¡c Ä‘áº·c trÆ°ng (features) lÃªn má»™t lá»›p/nhÃ£n (class/label) cá»¥ thá»ƒ cho tá»«ng máº«u (sample). 
 
- *Táº¡i sao má»™t logit chá»‰ áº£nh hÆ°á»Ÿng tá»›i má»™t máº«u nháº¥t Ä‘á»‹nh?* Há»“i quy logistic chá»‰ cÃ³ má»™t bá»™ tham sá»‘ duy nháº¥t, nÃªn má»—i máº«u chá»‰ cÃ³ má»™t logit Ä‘á»ƒ táº¡o ra chÃ­nh xÃ¡c má»™t xÃ¡c suáº¥t. Trong khi Ä‘Ã³, má»—i lá»›p trong há»“i quy softmax cÃ³ riÃªng má»™t bá»™ tham sá»‘ Ä‘á»ƒ tÃ­nh toÃ¡n má»™t Ä‘iá»ƒm sÃ´ thÃ´ cho chÃ­nh nÃ³, tá»« Ä‘Ã³ hÃ¬nh thÃ nh phÃ¢n phá»‘i xÃ¡c suáº¥t trÃªn nhiá»u lá»›p.

- *HÃ m softmax Ä‘Ã³ng vai trÃ² gÃ¬ trong mÃ´ hÃ¬nh nÃ y?* HÃ m softmax nháº­n Ä‘iá»ƒm sá»‘ thÃ´ Ä‘á»ƒ cho ra xÃ¡c suáº¥t cá»§a cÃ¡c lá»›p sao cho tá»•ng cá»§a táº¥t cáº£ xÃ¡c suáº¥t trÃªn má»™t máº«u báº±ng $1$. Nhá»¯ng xÃ¡c suáº¥t nÃ y mÃ´ táº£ sá»± tin tÆ°á»Ÿng cá»§a mÃ´ hÃ¬nh dÃ nh cho tá»«ng lá»›p. Lá»›p Ä‘Æ°á»£c dá»± Ä‘oÃ¡n chÃ­nh lÃ  lá»›p cÃ³ xÃ¡c suáº¥t lá»›n nháº¥t. 

## 2.2. CÃ´ng thá»©c
Cho má»™t táº­p há»£p Ä‘iá»ƒm dá»¯ liá»‡u vá»›i $N$ Ä‘áº·c trÆ°ng vÃ  $m$ lá»›p. ChÃºng ta sáº½ vectÆ¡ hÃ³a dá»¯ liá»‡u Ä‘áº§u vÃ o vÃ  Ä‘áº§u ra Ä‘á»ƒ thuáº­n tiá»‡n kÃ½ hiá»‡u, tÃ­nh toÃ¡n, vÃ  láº­p trÃ¬nh. 

![huong.png](/static/uploads/20251125_105039_234896f6.png)

### 2.2.1. Äiá»ƒm sá»‘ thÃ´ - Logits
Ta cÆ¡ báº£n Ä‘Ã£ hiá»ƒu Ã½ tÆ°á»Ÿng cá»§a cÃ¡c Ä‘iá»ƒm sá»‘ thÃ´ vÃ  vai trÃ² cá»§a chÃºng trong há»“i quy softmax. *Váº­y logits Ä‘Æ°á»£c thá»±c sá»± Ä‘Æ°a vÃ o mÃ´ hÃ¬nh nhÆ° tháº¿ nÃ o?* ChÃºng ta sáº½ cÃ¹ng xÃ¢y biá»ƒu thá»©c toÃ¡n há»c cho logits theo tá»«ng bÆ°á»›c cá»¥ thá»ƒ nhÆ° sau. 

- **Má»™t máº«u, má»™t lá»›p:** 

    Vá»›i $j \in \{1, \cdots, N\}$ vÃ  $i \in \{0, \cdots, m - 1\}$, ta cÃ³ 
    - $x^{(k)}_j$ lÃ  Ä‘áº·c trÆ°ng Ä‘áº§u vÃ o thá»© $j$ cá»§a máº«u $k$,
    - $w_{ij}$ lÃ  tham sá»‘ Ä‘áº·c trÆ°ng thá»© $j$ cá»­a lá»›p $i$,
    - $w_{i0}$ lÃ  há»‡ sá»‘ nghiÃªng cá»§a lá»›p $i$,
    - $\mathbf{x}^{(k)} = \begin{pmatrix} 1 \\ x^{(k)}_1 \\ \cdots \\ x^{(k)}_n \end{pmatrix} \in \mathbb{R}^{n+1}$ lÃ  vectÆ¡ Ä‘áº§u vÃ o kÃ¨m há»‡ sá»‘ nghiÃªng cá»§a máº«u $k$, 
    - $\mathbf{\theta}_i = \begin{pmatrix} w_{i0} \\ w_{i1} \\ \cdots \\ w_{in} \end{pmatrix} \in \mathbb{R}^{n+1}$ lÃ  vectÆ¡ tham sá»‘ kÃ¨m há»‡ sá»‘ nghiÃªng cá»§a lá»›p $i$, vÃ 
    - $z^{(k)}_i$ lÃ  logit cá»§a lá»›p $i$.

    PhÆ°Æ¡ng trÃ¬nh tuyáº¿n tÃ­nh biáº¿n vectÆ¡ máº«u $\mathbf{x}^{(k)}$ thÃ nh má»™t logit cá»§a lá»›p $i$ nhÆ° sau

    $$
    z^{(k)}_i = w_{i1} x^{(k)}_1 + \cdots + w_{in} x^{(k)}_n + w_{i0} = \sum_{j=0}^{n} w_{ij}x^{(k)}_j = \mathbf{\theta}_i^T \mathbf{x}^{(k)}.
    $$

- **Má»™t máº«u, $m$ lá»›p:** 
    
    Ta sáº½ sá»­ dá»¥ng láº¡i vectÆ¡ máº«u $\mathbf{x}^{(k)} \in \mathbb{R}^{n+1}$ á»Ÿ trÃªn. Vá»›i máº«u nÃ y, ta cáº§n má»™t vectÆ¡ lÆ°u trá»¯ cÃ¡c logits cá»§a táº¥t cáº£ cÃ¡c lá»›p vÃ  má»™t ma tráº­n lÆ°u trá»¯ cÃ¡c bá»™ trá»ng sá»‘. NhÆ° váº­y, ta cÃ³
    
    - $\mathbf{z}^{(k)} \in \mathbb{R}^{m \times 1}$ lÃ  vectÆ¡ logit trong Ä‘Ã³ má»—i thÃ nh pháº§n lÃ  $z^{(k)}_i$, vÃ 
    - $\Theta \in \mathbb{R}^{(n+1) \times m}$ lÃ  ma tráº­n trá»ng sá»‘ cÃ³ há»‡ sá»‘ nghiÃªng trong Ä‘Ã³ má»—i cá»™t lÃ  $\mathbf{\theta}_i$. 

    Khi Ä‘Ã³, cÃ¡c Ä‘iá»ƒm sá»‘ thÃ´ cho táº¥t cáº£ cÃ¡c lá»›p cá»§a máº«u nÃ y lÃ 

    $$
    \mathbf{z}^{(k)} = \Theta^T \mathbf{x}^{(k)}.
    $$

- **$K$ máº«u, $m$ lá»›p:**

    Ta sáº½ sá»­ dá»¥ng láº¡i ma tráº­n trá»ng sá»‘ $\Theta \in \mathbb{R}^{(n+1) \times m}$. NgoÃ i ra, ta cáº§n má»™t ma tráº­n lÆ°u trá»¯ cÃ¡c máº«u vÃ  má»™t ma tráº­n khÃ¡c lÆ°u trá»¯ logits tÆ°Æ¡ng á»©ng. Ta cÃ³

    - $X \in \mathbb{R}^{K \times (n+1)}$ lÃ  ma tráº­n máº«u trong Ä‘Ã³ má»—i dÃ²ng biá»ƒu diá»…n má»™t máº«u $\mathbf{x}^{(k)} \in \mathbb{R}^{n+1}$, vÃ  
    - $Z \in \mathbb{R}^{K \times m}$ lÃ  ma tráº­n logit trong Ä‘Ã³ má»—i thÃ nh pháº§n lÃ  logit cá»§a máº«u vÃ  lá»›p tÆ°Æ¡ng á»©ng.

    Khi Ä‘Ã³, ma tráº­n logit cho $K$ máº«u vÃ  $m$ lá»›p lÃ 

    $$
    Z = X \Theta.
    $$

### 2.2.2. Softmax Function
- *Táº¡i sao chá»‰ dá»±a vÃ o logit thÃ´i lÃ  chÆ°a Ä‘á»§?* Máº·c dÃ¹ cÃ¡c Ä‘iá»ƒm sá»‘ thÃ´ pháº£n Ã¡nh má»©c Ä‘á»™ áº£nh hÆ°á»Ÿng cá»§a cÃ¡c Ä‘áº·c trÆ°ng Ä‘áº§u vÃ o lÃªn má»™t lá»›p cá»¥ thá»ƒ nhÆ°ng chÃºng khÃ´ng thá»ƒ hiá»‡n Ä‘Æ°á»£c má»‘i tÆ°Æ¡ng quan giá»¯a cÃ¡c lá»›p vá»›i nhau. Do Ä‘Ã³, ta cáº§n biáº¿n cÃ¡c logit thÃ nh xÃ¡c suáº¥t Ä‘á»ƒ Ä‘áº¡t Ä‘Æ°á»£c má»¥c Ä‘Ã­ch ban Ä‘áº§u. 

- *Táº¡i sao cáº§n hÃ m softmax?* CÃ¡c logit lÃ  cÃ¡c sá»‘ thá»±c, cÃ³ thá»ƒ Ã¢m, dÆ°Æ¡ng, hoáº·c báº±ng $0$. Tuy nhiÃªn, sá»‘ Ã¢m khÃ´ng thá»ƒ tá»± biá»ƒu thá»‹ Ä‘Æ°á»£c má»™t xÃ¡c suáº¥t theo cÃ¡ch thÃ´ng thÆ°á»ng. ChÃ­nh vÃ¬ tháº¿, ta nÃªn dÃ¹ng hÃ m softmax Ä‘á»ƒ Ä‘áº£m báº£o tÃ­nh khÃ´ng Ã¢m. HÃ m softmax cÃ³ thá»ƒ biáº¿n Ä‘iá»ƒm sá»‘ thÃ´ thÃ nh cÃ¡c xÃ¡c suáº¥t cÃ³ Ã½ nghÄ©a vá»›i tá»•ng báº±ng $1$. Káº¿t quáº£ thá»ƒ hiá»‡n Ä‘á»™ máº¡nh cá»§a lá»›p tÆ°Æ¡ng á»©ng so vá»›i cÃ¡c lá»›p khÃ¡c.

- **HÃ m softmax:**
    $$
    softmax(z) = \frac{e^z}{\sum_c e^{z_c}}
    $$
- **HÃ m softmax trong há»“i quy softmax:**
    Cho máº«u $\mathbf{x}^{(k)}$ vá»›i vectÆ¡ logit $\mathbf{z}^{(k)}$, hÃ m softmax tÃ­nh xÃ¡c suáº¥t cá»§a lá»›p $i$ nhÆ° sau  

    $$
    \displaystyle \hat{y_i}^{(k)} = p(y=i | \mathbf{z}^{(k)}) = \frac{e^{{z^{(k)}_i}}}{\sum_{c=0}^{m-1}e^{z^{(k)}_c}}.
    $$

    CÃ´ng thá»©c nÃ y thá»a mÃ£n cÃ¡c yÃªu cáº§u:  
    - táº¥t cáº£ cÃ¡c xÃ¡c suáº¥t Ä‘á»u khÃ´ng Ã¢m nhá» hÃ m mÅ©,
    - tá»•ng cá»§a táº¥t cáº£ cÃ¡c xÃ¡c suáº¥t cho má»™t máº«u báº±ng $1$, vÃ  
    - xÃ¡c suáº¥t vÃ  logit tá»‰ lá»‡ thuáº­n vá»›i nhau.

- **VÃ­ dá»¥:**
    XÃ©t má»™t máº«u vá»›i Ä‘iá»ƒm sá»‘ thÃ´ $\mathbf{z} = \begin{pmatrix} z_0 \\ z_1 \\ z_2 \end{pmatrix} = \begin{pmatrix} 2.0 \\ 1.0 \\ 0.1 \end{pmatrix}$. Ãp dá»¥ng hÃ m mÅ© cho má»—i logit, ta cÃ³ 
    
    $$
    e^{2.0} = 7.3891, \quad e^{1.0} = 2.7183, \quad e^{0.1} = 1.1052,
    $$
    vÃ 
    $$
    \sum_{c = 0}^{2} e^{z_c} = e^{2.0} + e^{1.0} + e^{0.1} = 7.3891 + 2.7183 + 1.1052 = 11.2126.
    $$

    XÃ¡c suáº¥t cá»§a lá»›p $0$ Ä‘Æ°á»£c dá»± Ä‘oÃ¡n lÃ  

    $$
    \hat{y_0} = p(y = 0 | \mathbf{z}) = \frac{e^{2.0}}{\sum_{c = 0}^{2} e^{z_c}} = \frac{7.3891}{11.2126} \approx 0.6590.
    $$

    TÃ­nh tÆ°Æ¡ng tá»±, ta cÃ³ vectÆ¡ xuáº¥t cho lá»›p $i$ lÃ  

    $$
    \hat{y} = \begin{pmatrix} \hat{y_0} \\ \hat{y_1} \\ \hat{y_2} \end{pmatrix} \approx \begin{pmatrix} 0.6590 \\ 0.2424 \\ 0.0986 \end{pmatrix}
    $$

## 2.3. Quy trÃ¬nh huáº¥n luyá»‡n mÃ´ hÃ¬nh há»“i quy softmax
Má»¥c tiÃªu cá»§a mÃ´ hÃ¬nh há»“i quy softmax lÃ  Ä‘iá»u chá»‰nh tham sá»‘ sao cho xÃ¡c suáº¥t Ä‘Æ°á»£c dá»± Ä‘oÃ¡n chÃ­nh xÃ¡c nháº¥t cÃ³ thá»ƒ. 

- **Forward propagation:**
    - BÆ°á»›c 1: TÃ­nh logit báº±ng phÃ©p biáº¿n Ä‘á»•i tuyáº¿n tÃ­nh.
    - BÆ°á»›c 2: Ãp dá»¥ng hÃ m softmax Ä‘á»ƒ chuyá»ƒn logit thÃ nh phÃ¢n phá»‘i xÃ¡c suáº¥t. Lá»›p cÃ³ xÃ¡c suáº¥t cao nháº¥t lÃ  lá»›p dá»± Ä‘oÃ¡n.

- **Backward propagation:** (chi tiáº¿t á»Ÿ pháº§n sau)
    - BÆ°á»›c 3: TÃ­nh hÃ m máº¥t mÃ¡t (cross-entropy loss) Ä‘á»ƒ kiá»ƒm tra Ä‘á»™ há»£p giá»¯a nhÃ£n dá»± Ä‘oÃ¡n vÃ  nhÃ£n tháº­t.
    - BÆ°á»›c 4: TÃ­nh Ä‘áº¡o hÃ m (gradient) giÃºp xÃ¡c Ä‘á»‹nh cÃ¡c tham sá»‘ cáº§n thay Ä‘á»•i nhÆ° tháº¿ nÃ o Ä‘á»ƒ giáº£m hÃ m máº¥t mÃ¡t.
    - BÆ°á»›c 5: Cáº­p nháº­t tham sá»‘ báº±ng gradient descent.

Ta láº·p láº¡i quÃ¡ trÃ¬nh nÃ y cho cÃ¡c epoch tá»›i khi hÃ m máº¥t mÃ¡t há»™i tá»¥ vÃ  dá»± Ä‘oÃ¡n á»•n Ä‘á»‹nh vÃ  chÃ­nh xÃ¡c.

## 2.4. Ghi chÃº
TrÆ°á»›c khi káº¿t thÃºc pháº§n nÃ y, hÃ£y cÃ¹ng Ä‘iá»ƒm láº¡i má»™t vÃ i lÆ°u Ã½ quan trá»ng vá» mÃ´ hÃ¬nh há»“i quy softmax.

### 2.4.1. Æ¯u Ä‘iá»ƒm
- MÃ´ hÃ¬nh dá»… hiá»ƒu vÃ  dá»… diá»…n giáº£i káº¿t quáº£.
- MÃ´ hÃ¬nh Ä‘Æ°á»£c triá»ƒn khai vÃ  huáº¥n luyá»‡n hiá»‡u quáº£.
- HÃ m hÃ m máº¥t mÃ¡t lÃ  hÃ m lá»“i (convex) nÃªn Ä‘áº£m báº£o tÃ¬m Ä‘Æ°á»£c cá»±c tiá»ƒu toÃ n cá»¥c (the global minimum) mÃ  khÃ´ng gáº·p váº¥n Ä‘á» vá» cá»¥c tiá»ƒu cá»¥c bá»™ (local minima).

### 2.4.2. NhÆ°á»£c Ä‘iá»ƒm
- **TrÃ n sá»‘:**
    Khi logit quÃ¡ lá»›n, sá»‘ mÅ© $e^{z_i}$ cÃ³ thá»ƒ vÆ°á»£t quÃ¡ giÃ¡ trá»‹ lá»›n nháº¥t mÃ  kiá»ƒu dá»¯ liá»‡u cÃ³ thá»ƒ chá»©a Ä‘Æ°á»£c. Náº¿u khÃ´ng Ä‘Æ°á»£c giáº£i quyáº¿t triá»‡t Ä‘á»ƒ thÃ¬ cÃ³ thá»ƒ dáº«n Ä‘áº¿n tÃ¬nh tráº¡ng sai sá»‘. Äá»ƒ trÃ¡nh tÃ¬nh tráº¡ng nÃ y, ta cÃ³ thá»ƒ giá»›i háº¡n sá»‘ mÅ© báº±ng $m = \underset{j}{max} z_i$ mÃ  váº«n giá»¯ nguyÃªn káº¿t quáº£. Váº­y, ta cÃ³

    $$
    \hat{y_i} = \frac{e^{z_i-m}}{\sum_c e^{z_c-m}}
    $$

    Ta tháº¥y trá»« Ä‘i má»™t háº±ng sá»‘ trÃªn mÅ© (logits) khÃ´ng lÃ m thay Ä‘á»•i káº¿t quáº£ cá»§a hÃ m softmax vÃ¬ $e^m$ bá»‹ khá»­ trong phÃ¢n sá»‘. 

- **Quan há»‡ phi tuyáº¿n:**
    Há»“i quy softmax lÃ  mÃ´ hÃ¬nh má»™t táº§ng/lá»›p vá»›i hÃ m giáº£ thuyáº¿t lÃ  phÃ©p biáº¿n Ä‘á»•i tuyáº¿n tÃ­nh. TÃ­nh cháº¥t nÃ y khiáº¿n cho mÃ´ hÃ¬nh nháº¡y cáº£m vá»›i cÃ¡c ngoáº¡i lá»‡ (outliers) cÅ©ng nhÆ° khÃ³ xá»­ lÃ½ má»‘i quan há»‡ phi tuyáº¿n tÃ­nh má»™t cÃ¡ch hiá»‡u quáº£. Äá»ƒ giáº£i quyáº¿t váº¥n Ä‘á» nÃ y, ta cáº§n cÃ³ dáº¡ng máº¡ng nhiá»u táº§ng nhÆ° Multilayer Perceptron (MLP).



<!-- 
# 2. Softmax Regression 

## 2.1. Main Idea
- Softmax Regression, which handles multi-class classification problems, is a generalization of logistic regression. Instead of using the sigmoid function, softmax regression employs the softmax function to turn raw scores into an explicit probability distribution across all classes. 

- *Where do these raw scores (also known as logits) come from?* Like in logistic regression, a logit results from a linear combination which is a weighted sum of input features and a bias. Each of them in softmax regression represents the features' influence on a particular class. 

- *Why does one logit only support a specific class?* In logistic regression, each sample produces just one probability as there is only one score derived from a single set of parameters. In contrast, each class in softmax regression has its own set of parameters to create one raw score thereby generate a probability distribution over multiple classes. 

- *How does the softmax function contribute to the model?* A raw score is fed to the softmax function to compute probabilities over all the classes for one sample. Such probabilities sum to $1$, which describes how strongly the model favors one class over the rest. Thus, the predicted class is simply the one with the highest probability within this distribution.

## 2.2. Formula
Consider a dataset of $N$ features and $m$ classes. For mathematical convenience and more efficient computation, we'll vectorize input and output.

<figure>
    <img src="softmax_regression_forward_propagation.png" alt="forward propagation for softmax regression">
    <figcaption>Forward propagation with raw scores/logits and softmax function in Softmax Regression</figcaption>
</figure>

### 2.2.1. Raw Scores (Logits)
- We understand the idea of raw scores and their contribution to softmax regression. The question now is how these logits technically fit into the model. Let's formalize the mathematical defintion of logits.

- **One sample, one particular class:** 

    For $j \in \{1, \cdots, N\}$ and $i \in \{0, \cdots, m - 1\}$, let 
    - $x^{(k)}_j$ be the $k^{th}$ sample's $j^{th}$ input feature,
    - $w_{ij}$ be the $j^{th}$ feature's parameter for class $i$,
    - $w_{i0}$ be the bias term for class $i$,
    - $\mathbf{x}^{(k)} = \begin{pmatrix} 1 \\ x^{(k)}_1 \\ \cdots \\ x^{(k)}_n \end{pmatrix} \in \mathbb{R}^{n+1}$ be the $k^{th}$ sample's input vector with bias included, 
    - $\mathbf{\theta}_i = \begin{pmatrix} w_{i0} \\ w_{i1} \\ \cdots \\ w_{in} \end{pmatrix} \in \mathbb{R}^{n+1}$ be the weight-with-bias vector for class $i$, and
    - $z^{(k)}_i$ be the raw score (logit) for class $i$.

    A linear equation transforms the sample vector $\mathbf{x}^{(k)}$ into one raw score (logit) for class $i$ as follows.

    $$
    z^{(k)}_i = w_{i1} x^{(k)}_1 + \cdots + w_{in} x^{(k)}_n + w_{i0} = \sum_{j=0}^{n} w_{ij}x^{(k)}_j = \mathbf{\theta}_i^T \mathbf{x}^{(k)}.
    $$

- **One sample, $m$ classes:** 
    
    We'll use the same sample vector $\mathbf{x}^{(k)} \in \mathbb{R}^{n+1}$ as above. We'll introduce a vector for storing logits for all classes and a matrix for storing all sets of parameters. Consider
    
    - $\mathbf{z}^{(k)} \in \mathbb{R}^{m \times 1}$ is the logit vector where each entry is $z^{(k)}_i$, and
    - $\Theta \in \mathbb{R}^{(n+1) \times m}$ is the weight-with-bias matrix where each column is $\mathbf{\theta}_i$. 

    The logits for all classes are then given by

    $$
    \mathbf{z}^{(k)} = \Theta^T \mathbf{x}^{(k)}.
    $$

- **$K$ samples, $m$ classes:**

    We'll use the same parameter matrix $\Theta \in \mathbb{R}^{(n+1) \times m}$. We'll need a matrix to store all interested samples and another to store their logits. Suppose

    - $X \in \mathbb{R}^{K \times (n+1)}$ is the sample matrix where each row represents a sample $\mathbf{x}^{(k)} \in \mathbb{R}^{n+1}$, and 
    - $Z \in \mathbb{R}^{K \times m}$ is a logit matrix where each entry contains the logit of the corresponding sample-class pair.

    Then, the logit matrix for $K$ samples and $m$ classes is defined as

    $$
    Z = X \Theta.
    $$

### 2.2.2. Softmax Function
- *Why are raw scores not enough?* Though raw scores reflect how much input features support a specific class, they do not show where this class stands relative to the others. So converting logits into probabilities is necessary. 

- *Why do we need the softmax function?* Notice that logits can be negative, so they cannot represent valid probabilities themselves. This is why we use the softmax function, which is a method that can transform raw scores into meaningful probabilities and sum to $1$. The results express each class's strength in comparison to the rest. 

- **General softmax function:**
    $$
    softmax(z) = \frac{e^z}{\sum_c e^{z_c}}
    $$
- **Softmax function in softmax regression:**
    For a sample $\mathbf{x}^{(k)}$ with logits $\mathbf{z}^{(k)}$, the softmax function computes the probability of class $i$ as  

    $$
    \displaystyle \hat{y_i}^{(k)} = p(y=i | \mathbf{z}^{(k)}) = \frac{e^{{z^{(k)}_i}}}{\sum_{c=0}^{m-1}e^{z^{(k)}_c}}.
    $$

    This formula ensures: 
    - all the probabilities are non-negative via the exponential function,
    - all probabilities for one sample sum to $1$, and 
    - higher logits yield higher probabilities.

- **Example:**
    Suppose a sample has raw scores $\mathbf{z} = \begin{pmatrix} z_0 \\ z_1 \\ z_2 \end{pmatrix} = \begin{pmatrix} 2.0 \\ 1.0 \\ 0.1 \end{pmatrix}$. Applying the exponential function on each logit, we get

    $$
    e^{2.0} = 7.3891, \quad e^{1.0} = 2.7183, \quad e^{0.1} = 1.1052,
    $$
    and
    $$
    \sum_{c = 0}^{2} e^{z_c} = e^{2.0} + e^{1.0} + e^{0.1} = 7.3891 + 2.7183 + 1.1052 = 11.2126.
    $$

    The estimated probability for class $0$ is 

    $$
    \hat{y_0} = p(y = 0 | \mathbf{z}) = \frac{e^{2.0}}{\sum_{c = 0}^{2} e^{z_c}} = \frac{7.3891}{11.2126} \approx 0.6590.
    $$

    Thus, the estimated probability vector for the sample is 

    $$
    \hat{y} = \begin{pmatrix} \hat{y_0} \\ \hat{y_1} \\ \hat{y_2} \end{pmatrix} \approx \begin{pmatrix} 0.6590 \\ 0.2424 \\ 0.0986 \end{pmatrix}
    $$

## 2.3. Training
Now that concept is in place, let's shift our attention to how softmax regression is trained. The goal is to adjust the parameters so that the probabilities are estimated as accurately as possible.

- **Forward propagation:**
    - *Step 1:* Compute logits using the linear transformation.
    - *Step 2:* Apply the softmax function to obtain a probability distribution from logits. Those with the highest probabilities are the predicted classes.

- **Backward propagation:** (covered in later sections)
    - *Step 3:* Compute loss to measure how well the predictions match the true labels.
    - *Step 4:* Compute gradients that determine how parameters should change to reduce the loss.
    - *Step 5:* Update parameters using gradient descent to better predictions.

This process repeats over multiple epochs until the loss converges and specifically the predictions become consistently accurate.

## 2.4. Notes
Before we wrap up this section, here are a few key notes about the softmax regression model. 

### 2.4.1. Advantages
- It is easy to understand and interpret the outcomes.
- It is efficient to implement and train.
- The loss function is convex, which leaves the global mimum but local minima.

### 2.4.2. Disadvantages
- **Numerical overflow:**
    When logits are too large, the exponential $e^{z_i}$ can overflow, which leads to computational inaccuracy if not carefully handled. To avoid this situation, we can limit the exponent by $m = \underset{j}{max} z_i$ and keep the result the same. Therefore,

    $$
    \hat{y_i} = \frac{e^{z_i-m}}{\sum_c e^{z_c-m}}.
    $$

    Note that subtracting a constant $m$ from exponents (logits) does not change the result of the softmax function as $e^m$ is cancelled out.

- **Non-linear relationship:**
    Softmax regression is a single-layered model with the hypothesis function being a linear transformation. Thus, it not only can be sensitive to outliers but struggles handling non-linear relationship efficiently as well. This is when Multilayer Perceptron (MLP) comes into play. 

  -->
  
  ## CÆ¡ cháº¿ tá»‘i Æ°u hÃ³a sai sá»‘ phÃ¢n loáº¡i qua hÃ m Cross Entropy

### 1. KhÃ¡i niá»‡m
Trong cÃ¡c bÃ i toÃ¡n phÃ¢n loáº¡i (Classification), má»¥c tiÃªu cá»§a mÃ´ hÃ¬nh khÃ´ng pháº£i lÃ  dá»± Ä‘oÃ¡n má»™t giÃ¡ trá»‹ sá»‘ há»c, mÃ  lÃ  Æ°á»›c lÆ°á»£ng má»™t **phÃ¢n phá»‘i xÃ¡c suáº¥t** Ä‘á»ƒ xÃ¡c Ä‘á»‹nh dá»¯ liá»‡u thuá»™c vá» lá»›p nÃ o.

**Cross Entropy Loss** (hay Log Loss) lÃ  thÆ°á»›c Ä‘o Ä‘á»‹nh lÆ°á»£ng sá»± khÃ¡c biá»‡t giá»¯a hai phÃ¢n phá»‘i xÃ¡c suáº¥t:
1.  **PhÃ¢n phá»‘i thá»±c táº¿ ($y$):** NhÃ£n Ä‘Ãºng cá»§a dá»¯ liá»‡u (thÆ°á»ng lÃ  tuyá»‡t Ä‘á»‘i, vÃ­ dá»¥: 100% lÃ  mÃ¨o, 0% lÃ  chÃ³).
2.  **PhÃ¢n phá»‘i dá»± Ä‘oÃ¡n ($\hat{y}$):** Äáº§u ra cá»§a mÃ´ hÃ¬nh sau khi Ä‘i qua hÃ m kÃ­ch hoáº¡t (Softmax hoáº·c Sigmoid), thá»ƒ hiá»‡n Ä‘á»™ tin cáº­y cá»§a mÃ´ hÃ¬nh Ä‘á»‘i vá»›i tá»«ng lá»›p.

GiÃ¡ trá»‹ Cross Entropy cÃ ng tháº¥p, phÃ¢n phá»‘i dá»± Ä‘oÃ¡n cÃ ng tiá»‡m cáº­n vá»›i phÃ¢n phá»‘i thá»±c táº¿, Ä‘á»“ng nghÄ©a vá»›i viá»‡c Ä‘á»™ chÃ­nh xÃ¡c cá»§a mÃ´ hÃ¬nh cÃ ng cao.

### 2. Ã nghÄ©a vá» máº·t toÃ¡n há»c
XÃ©t bÃ i toÃ¡n phÃ¢n loáº¡i Ä‘a lá»›p vá»›i $C$ lá»›p dá»¯ liá»‡u, hÃ m Loss Ä‘Æ°á»£c xÃ¡c Ä‘á»‹nh bá»Ÿi cÃ´ng thá»©c tá»•ng quÃ¡t:

$$L_{CE} = - \sum_{i=1}^{C} y_i \cdot \log(\hat{y}_i)$$

**CÃ¡c thÃ nh pháº§n:**
* $y_i$: NhÃ£n thá»±c táº¿ dÆ°á»›i dáº¡ng **One-hot vector**. Táº¡i vá»‹ trÃ­ lá»›p Ä‘Ãºng, $y_i = 1$, cÃ¡c vá»‹ trÃ­ cÃ²n láº¡i $y_i = 0$.
* $\hat{y}_i$: XÃ¡c suáº¥t dá»± Ä‘oÃ¡n cho lá»›p thá»© $i$.
* $\log$: Logarithm tá»± nhiÃªn.

Do tÃ­nh cháº¥t cá»§a One-hot vector (cÃ¡c pháº§n tá»­ sai Ä‘á»u báº±ng 0), cÃ´ng thá»©c thá»±c táº¿ khi tÃ­nh toÃ¡n cho má»™t máº«u dá»¯ liá»‡u chá»‰ táº­p trung vÃ o xÃ¡c suáº¥t cá»§a lá»›p Ä‘Ãºng ($y_{target} = 1$):

$$L = - \log(\hat{y}_{target})$$

Äiá»u nÃ y cÃ³ nghÄ©a lÃ  viá»‡c tá»‘i Æ°u hÃ³a Cross Entropy chÃ­nh lÃ  bÃ i toÃ¡n cá»±c Ä‘áº¡i hÃ³a Log-likelihood cá»§a lá»›p Ä‘Ãºng.



### 3. Diá»…n giáº£i
CÆ¡ cháº¿ cá»‘t lÃµi cá»§a Cross Entropy dá»±a trÃªn sá»± trá»«ng pháº¡t sai sá»‘ thÃ´ng qua hÃ m Logarithm ($-\log(x)$).

* **Khi mÃ´ hÃ¬nh dá»± Ä‘oÃ¡n Ä‘Ãºng vÃ  tá»± tin:** Náº¿u xÃ¡c suáº¥t dá»± Ä‘oÃ¡n cho lá»›p Ä‘Ãºng $\hat{y} \approx 1$, thÃ¬ $-\log(1) = 0$. Loss xáº¥p xá»‰ báº±ng 0, mÃ´ hÃ¬nh khÃ´ng bá»‹ pháº¡t.
* **Khi mÃ´ hÃ¬nh dá»± Ä‘oÃ¡n sai hoáº·c thiáº¿u tá»± tin:** Náº¿u xÃ¡c suáº¥t dá»± Ä‘oÃ¡n cho lá»›p Ä‘Ãºng $\hat{y} \rightarrow 0$, thÃ¬ $-\log(\hat{y}) \rightarrow +\infty$. GiÃ¡ trá»‹ Loss tÄƒng vá»t.

![nhan.webp](/static/uploads/20251125_105614_f151c9ad.webp)

Biá»ƒu Ä‘á»“ hÃ m $-\log(x)$ cho tháº¥y Ä‘á»™ dá»‘c (gradient) ráº¥t lá»›n khi $x$ gáº§n 0. Äiá»u nÃ y khiáº¿n cho giÃ¡ trá»‹ Ä‘áº¡o hÃ m ráº¥t cao, giÃºp thuáº­t toÃ¡n Gradient Descent Ä‘iá»u chá»‰nh trá»ng sá»‘ nhanh chÃ³ng khi mÃ´ hÃ¬nh Ä‘ang dá»± Ä‘oÃ¡n sai lá»‡ch quÃ¡ nhiá»u. NgÆ°á»£c láº¡i, khi dá»± Ä‘oÃ¡n Ä‘Ã£ khÃ¡ chÃ­nh xÃ¡c, Ä‘á»™ dá»‘c giáº£m dáº§n giÃºp mÃ´ hÃ¬nh há»™i tá»¥ á»•n Ä‘á»‹nh.

### 4. LÆ°u Ã½ khi triá»ƒn khai
Khi triá»ƒn khai vÃ  sá»­ dá»¥ng Cross Entropy, cáº§n chÃº Ã½ cÃ¡c Ä‘iá»ƒm ká»¹ thuáº­t sau Ä‘á»ƒ Ä‘áº£m báº£o tÃ­nh á»•n Ä‘á»‹nh:

* **So sÃ¡nh vá»›i MSE:** 
* **Váº¥n Ä‘á» log(0):** HÃ m log khÃ´ng xÃ¡c Ä‘á»‹nh táº¡i 0. Trong thá»±c táº¿ láº­p trÃ¬nh, cáº§n cá»™ng thÃªm má»™t giÃ¡ trá»‹ cá»±c nhá» $\epsilon$ (vÃ­ dá»¥ $1e^{-6}$) vÃ o $\hat{y}$ Ä‘á»ƒ trÃ¡nh lá»—i tÃ­nh toÃ¡n (`NaN`).
* **Káº¿t há»£p hÃ m kÃ­ch hoáº¡t:**
    * Vá»›i bÃ i toÃ¡n nhá»‹ phÃ¢n (2 lá»›p): Sá»­ dá»¥ng **Binary Cross Entropy** káº¿t há»£p vá»›i Ä‘áº§u ra **Sigmoid**.
    * Vá»›i bÃ i toÃ¡n Ä‘a lá»›p (>2 lá»›p): Sá»­ dá»¥ng **Cross Entropy** káº¿t há»£p vá»›i Ä‘áº§u ra **Softmax**.


# 4. CÃ¡c hÃ m loss khÃ¡c trong classification (ngoÃ i Categorical Cross-Entropy)

## 1. Ã tÆ°á»Ÿng chÃ­nh (Motivation)

Máº·c dÃ¹ Categorical Cross-Entropy lÃ  lá»±a chá»n máº·c Ä‘á»‹nh cho cÃ¡c mÃ´ hÃ¬nh phÃ¢n loáº¡i hiá»‡n Ä‘áº¡i, nhiá»u hÃ m loss cá»• Ä‘iá»ƒn tá»«ng giá»¯ vai trÃ² quan trá»ng trong lá»‹ch sá»­ phÃ¡t triá»ƒn cá»§a há»c mÃ¡y. Nhá»¯ng hÃ m loss nÃ y pháº£n Ã¡nh cÃ¡c quan Ä‘iá»ƒm khÃ¡c nhau vá» cÃ¡ch mÃ´ hÃ¬nh hoÃ¡ khÃ¡i niá»‡m â€œÄ‘Ãºng - saiâ€, má»©c Ä‘á»™ tin cáº­y cá»§a dá»± Ä‘oÃ¡n, vÃ  hÃ nh vi tá»‘i Æ°u hÃ³a ká»³ vá»ng.

Ba hÃ m loss kinh Ä‘iá»ƒn nháº¥t - **Zero-One Loss**, **Exponential Loss**, vÃ  **Hinge Loss** - lÃ  ná»n táº£ng Ä‘á»ƒ hiá»ƒu táº¡i sao Cross-Entropy (vÃ  Logistic Loss nÃ³i chung) trá»Ÿ thÃ nh chuáº©n má»±c hiá»‡n nay. ChÃºng Ä‘áº¡i diá»‡n cho ba triáº¿t lÃ½ há»c khÃ¡c nhau: tá»‘i Æ°u trá»±c tiáº¿p sai sá»‘, nháº¥n máº¡nh máº«u khÃ³, vÃ  tá»‘i Æ°u hÃ³a margin.

## 2. CÃ¡c hÃ m loss

### 2.1 Zero-One Loss - mÃ´ táº£ trá»±c tiáº¿p má»¥c tiÃªu classification

#### Trá»±c giÃ¡c

ÄÃ¢y lÃ  Ä‘á»‹nh nghÄ©a thuáº§n tÃºy nháº¥t cá»§a bÃ i toÃ¡n phÃ¢n loáº¡i:  
dá»± Ä‘oÃ¡n Ä‘Ãºng â†’ khÃ´ng bá»‹ pháº¡t,  
dá»± Ä‘oÃ¡n sai â†’ pháº¡t má»™t Ä‘Æ¡n vá»‹.  

Zero-One Loss khÃ´ng xÃ©t má»©c Ä‘á»™ sai Ã­t hay sai nhiá»u, chá»‰ quan tÃ¢m Ä‘áº§u ra cuá»‘i cÃ¹ng.

#### CÃ´ng thá»©c

$$
\mathcal{L}_{0-1}(y, \hat{y}) =
\begin{cases}
0 & \text{khi } \hat{y} = y \\
1 & \text{khi } \hat{y} \ne y
\end{cases}
$$

#### Ã nghÄ©a há»c thuáº­t

Zero-One Loss pháº£n Ã¡nh Ä‘Ãºng má»¥c tiÃªu classification, nhÆ°ng:

- khÃ´ng kháº£ vi,  
- khÃ´ng cho biáº¿t hÆ°á»›ng Ä‘iá»u chá»‰nh,  
- landscape tá»‘i Æ°u hÃ³a rá»i ráº¡c, ráº¥t nhiá»u Ä‘iá»ƒm káº¹t.

Do Ä‘Ã³, nÃ³ **khÃ´ng thá»ƒ dÃ¹ng Ä‘á»ƒ huáº¥n luyá»‡n báº±ng gradient descent**.  
ToÃ n bá»™ cÃ¡c loss nhÆ° Logistic, Hinge, Exponential Ä‘á»u lÃ  **surrogate** nháº±m xáº¥p xá»‰ Zero-One theo cÃ¡ch mÆ°á»£t hÆ¡n, tá»‘i Æ°u hÃ³a Ä‘Æ°á»£c.

---

### 2.2 Exponential Loss - nháº¥n máº¡nh máº¡nh máº½ vÃ o cÃ¡c máº«u khÃ³ (triáº¿t lÃ½ Boosting)

#### Trá»±c giÃ¡c

Exponential Loss mÃ´ phá»ng hÃ nh vi:  
**má»™t lá»—i sai Ä‘Æ°á»£c Ä‘Æ°a ra vá»›i sá»± â€œtá»± tin caoâ€ sáº½ bá»‹ pháº¡t ráº¥t máº¡nh**.

ÄÃ¢y chÃ­nh lÃ  cÆ¡ cháº¿ há»c cá»§a AdaBoost: táº­p trung ngÃ y cÃ ng nhiá»u vÃ o nhá»¯ng máº«u mÃ  mÃ´ hÃ¬nh Ä‘ang phÃ¢n loáº¡i sai.

#### CÃ´ng thá»©c

$$
\mathcal{L}_{\text{exp}}(y, f(x)) = e^{-y f(x)}
$$

Trong Ä‘Ã³:

- $y \in \{-1, +1\}$ lÃ  nhÃ£n nhá»‹ phÃ¢n,  
- $f(x)$ lÃ  logit hoáº·c score cá»§a mÃ´ hÃ¬nh.

#### Ã nghÄ©a há»c thuáº­t

- Náº¿u dá»± Ä‘oÃ¡n Ä‘Ãºng â†’ $y f(x)$ lá»›n â†’ hÃ m mÅ© ráº¥t nhá».  
- Náº¿u dá»± Ä‘oÃ¡n sai â†’ $y f(x) < 0$ â†’ loss tÄƒng theo cáº¥p sá»‘ nhÃ¢n.

Trong AdaBoost:

- máº«u sai cÃ³ loss lá»›n â†’ trá»ng sá»‘ tÄƒng,  
- dáº«n tá»›i viá»‡c Ä‘Æ°á»£c â€œhá»c ká»¹ hÆ¡nâ€ á»Ÿ cÃ¡c vÃ²ng tiáº¿p theo.

##### LiÃªn há»‡ vá»›i tá»‘i Æ°u hÃ³a margin 

Exponential Loss cÅ©ng tá»‘i Æ°u hÃ³a má»™t dáº¡ng margin, nhÆ°ng *khÃ´ng cÃ³ vÃ¹ng pháº³ng nhÆ° Hinge Loss*.  
VÃ¬ váº­y mÃ´ hÃ¬nh cÃ³ xu hÆ°á»›ng **khÃ´ng bao giá» dá»«ng tÄƒng Ä‘á»™ tin tÆ°á»Ÿng** trÃªn cÃ¡c máº«u dá»± Ä‘oÃ¡n Ä‘Ãºng, khiáº¿n boosting Ä‘Ã´i khi quÃ¡ táº­p trung vÃ o outlier hoáº·c nhiá»…u.

---

### 2.3 Hinge Loss - tá»‘i Æ°u hÃ³a margin (tÆ° duy SVM)

#### Trá»±c giÃ¡c

Hinge Loss khÃ´ng chá»‰ yÃªu cáº§u mÃ´ hÃ¬nh dá»± Ä‘oÃ¡n Ä‘Ãºng, mÃ  cÃ²n dá»± Ä‘oÃ¡n Ä‘Ãºng **vá»›i khoáº£ng cÃ¡ch an toÃ n (margin)** so vá»›i ranh giá»›i phÃ¢n chia.

â€œÄÃºng nhÆ°ng khÃ´ng cháº¯c cháº¯nâ€ váº«n bá»‹ xem lÃ  chÆ°a Ä‘áº¡t yÃªu cáº§u.

#### CÃ´ng thá»©c nhá»‹ phÃ¢n

$$
\mathcal{L}_{\text{hinge}}(y, f(x)) = \max(0,\; 1 - y f(x))
$$

#### Ã nghÄ©a há»c thuáº­t

- Náº¿u $y f(x) \ge 1$: Ä‘Ãºng vÃ  margin Ä‘á»§ lá»›n â†’ loss = 0  
- Náº¿u $0 < y f(x) < 1$: Ä‘Ãºng nhÆ°ng yáº¿u â†’ bá»‹ pháº¡t tuyáº¿n tÃ­nh  
- Náº¿u $y f(x) < 0$: sai â†’ pháº¡t máº¡nh  

Hinge Loss lÃ  ná»n táº£ng cá»§a **Support Vector Machine**, nÆ¡i má»¥c tiÃªu lÃ  **tá»‘i Ä‘a hÃ³a margin** Ä‘á»ƒ cáº£i thiá»‡n kháº£ nÄƒng tá»•ng quÃ¡t hÃ³a.

#### TÃ­nh kháº£ vi

Hinge Loss:

- **lá»“i (convex)** nhÆ°ng  
- **khÃ´ng kháº£ vi táº¡i Ä‘iá»ƒm $y f(x)=1$**.

Tuy nhiÃªn, nÃ³ **sub-differentiable**, vÃ  SGD Ã¡p dá»¥ng **subgradient** giá»‘ng ReLU.

#### Multiclass Hinge Loss

Sá»­ dá»¥ng cho phÃ¢n loáº¡i $K$ lá»›p:

$$
\mathcal{L}(s,y) = \sum_{j \ne y} \max(0,\; s_j - s_y + 1)
$$

ÄÃ¢y lÃ  phiÃªn báº£n thÆ°á»ng gá»i lÃ  **SVM Loss** hoáº·c **Crammer-Singer Loss**.

#### LiÃªn há»‡ vá»›i Logistic Loss / Cross-Entropy

- Logistic Loss lÃ  má»™t surrogate lá»“i, mÆ°á»£t (smooth), kháº£ vi toÃ n pháº§n.  
- VÃ¬ Logistic Loss á»•n Ä‘á»‹nh hÆ¡n vá» máº·t gradient, nÃ³ phÃ¹ há»£p hÆ¡n vá»›i deep learning so vá»›i Hinge Loss.  
- Cross-Entropy trong phÃ¢n loáº¡i Ä‘a lá»›p lÃ  má»Ÿ rá»™ng tá»± nhiÃªn cá»§a Logistic Loss.

---

## 3. So sÃ¡nh trá»±c quan báº±ng lá»i

- **Zero-One Loss**: dáº¡ng bÆ°á»›c nháº£y - khÃ´ng mÆ°á»£t.  
- **Hinge Loss**: tuyáº¿n tÃ­nh khi sai â†’ pháº³ng ngay khi Ä‘áº¡t margin.  
- **Logistic Loss**: Ä‘Æ°á»ng cong mÆ°á»£t, khÃ´ng cÃ³ Ä‘iá»ƒm gÃ£y.  
- **Exponential Loss**: giáº£m ráº¥t nhanh khi Ä‘Ãºng, tÄƒng ráº¥t máº¡nh khi sai.  

### Má»©c Ä‘á»™ mÆ°á»£t (smoothness)

$$
\text{Zero-One} \;<\; \text{Hinge} \;<\; \text{Logistic} \;<\; \text{Exponential}
$$

(Zero-One khÃ´ng kháº£ vi; Hinge cÃ³ Ä‘iá»ƒm khÃ´ng kháº£ vi; Logistic mÆ°á»£t hoÃ n toÃ n; Exponential cá»±c nháº¡y vá»›i lá»—i.)

---

## 4. Pháº§n code

### Zero-One Loss (dÃ¹ng Ä‘á»ƒ Ä‘Ã¡nh giÃ¡, khÃ´ng dÃ¹ng Ä‘á»ƒ huáº¥n luyá»‡n)

    import torch

    # y_true: (batch,)
    # y_pred_logits: (batch, num_classes)
    def zero_one_loss(y_true, y_pred_logits):
        pred = y_pred_logits.argmax(dim=1)
        return (pred != y_true).float().mean()

### Exponential Loss

    import torch
    def exponential_loss(logits, target):
        return torch.exp(- target * logits).mean()

### Multiclass Hinge Loss

    import torch
    def multiclass_hinge_loss(scores, target):
        batch, K = scores.shape
        correct = scores[torch.arange(batch), target].unsqueeze(1)
        margins = torch.clamp(scores - correct + 1, min=0)
        margins[torch.arange(batch), target] = 0
        return margins.mean()

# HÆ°á»›ng Dáº«n CÃ i Äáº·t Softmax Regression Tá»« Zero Báº±ng Python

Trong bÃ i viáº¿t nÃ y, chÃºng ta sáº½ tá»± xÃ¢y dá»±ng **Softmax Regression** â€” mÃ´ hÃ¬nh phÃ¢n loáº¡i Ä‘a lá»›p â€” hoÃ n toÃ n báº±ng **NumPy**.

Má»i khÃ¡i niá»‡m Ä‘á»u dá»±a trÃªn tÃ i liá»‡u **Softmax Regression** báº¡n Ä‘Ã£ táº£i lÃªn.

BÃ i viáº¿t sáº½ giÃºp báº¡n:

- Hiá»ƒu *one-hot encoding*  
- Tá»± xÃ¢y *Softmax function*  
- TÃ­nh *cross-entropy loss*  
- TÃ­nh *gradient* báº±ng cÃ´ng thá»©c tá»« tÃ i liá»‡u  
- Cáº­p nháº­t tham sá»‘ báº±ng *gradient descent*  

![chien.png](/static/uploads/20251125_105713_7ceff515.png)

# Giai Ä‘oáº¡n Huáº¥n luyá»‡n (Training Phase)

1. **Khá»Ÿi táº¡o trá»ng sá»‘ (Initialize weights)**  
   Báº¯t Ä‘áº§u báº±ng cÃ¡ch gÃ¡n giÃ¡ trá»‹ ban Ä‘áº§u cho cÃ¡c trá»ng sá»‘ vÃ  bias. ThÆ°á»ng dÃ¹ng giÃ¡ trá»‹ ngáº«u nhiÃªn hoáº·c theo má»™t phÆ°Æ¡ng phÃ¡p chuáº©n hÃ³a.

2. **Chá»n má»™t máº«u (x, y) tá»« dá»¯ liá»‡u huáº¥n luyá»‡n (Pick a sample)**  
   Láº¥y tá»«ng cáº·p dá»¯ liá»‡u Ä‘áº§u vÃ o vÃ  nhÃ£n tÆ°Æ¡ng á»©ng Ä‘á»ƒ tÃ­nh toÃ¡n.

3. **TÃ­nh Ä‘áº§u ra dá»± Ä‘oÃ¡n $( \hat{y} )$ (Compute output)**  
   DÃ¹ng trá»ng sá»‘ hiá»‡n táº¡i Ä‘á»ƒ dá»± Ä‘oÃ¡n Ä‘áº§u ra tá»« máº«u dá»¯ liá»‡u. ÄÃ¢y lÃ  bÆ°á»›c forward propagation.

4. **TÃ­nh loss (Compute loss)**  
   So sÃ¡nh giÃ¡ trá»‹ dá»± Ä‘oÃ¡n $( \hat{y} )$ vá»›i giÃ¡ trá»‹ tháº­t y Ä‘á»ƒ tÃ­nh hÃ m máº¥t mÃ¡t (loss).

5. **TÃ­nh Ä‘áº¡o hÃ m (Compute derivative)**  
   TÃ­nh gradient cá»§a loss theo cÃ¡c trá»ng sá»‘. ÄÃ¢y lÃ  bÆ°á»›c quan trá»ng Ä‘á»ƒ biáº¿t cáº§n Ä‘iá»u chá»‰nh trá»ng sá»‘ nhÆ° tháº¿ nÃ o.

6. **Cáº­p nháº­t tham sá»‘ (Update parameters)**  
   Sá»­ dá»¥ng gradient vÃ  learning rate Ä‘á»ƒ cáº­p nháº­t trá»ng sá»‘, nháº±m giáº£m loss.

7. **Láº·p láº¡i tá»« bÆ°á»›c 2 cho máº«u tiáº¿p theo (Repeat from step 2)**  
   Tiáº¿p tá»¥c vá»›i cÃ¡c máº«u khÃ¡c cho Ä‘áº¿n khi toÃ n bá»™ dá»¯ liá»‡u Ä‘Æ°á»£c huáº¥n luyá»‡n hoáº·c Ä‘áº¡t Ä‘iá»u kiá»‡n dá»«ng.

ğŸ’¡ **ChÃº thÃ­ch thÃªm:** QuÃ¡ trÃ¬nh nÃ y láº·p Ä‘i láº·p láº¡i nhiá»u láº§n (epochs) trÃªn toÃ n bá»™ táº­p dá»¯ liá»‡u Ä‘á»ƒ mÃ´ hÃ¬nh há»c Ä‘Æ°á»£c cÃ¡c Ä‘áº·c trÆ°ng vÃ  giáº£m lá»—i dá»± Ä‘oÃ¡n.


## B1: Chuáº©n bá»‹ dá»¯ liá»‡u vÃ  One-hot Encoding

TÃ i liá»‡u chá»‰ ra ráº±ng cross-entropy nhiá»u lá»›p Ä‘Æ°á»£c viáº¿t gá»n báº±ng one-hot:

$$
L = -\sum_{j=1}^{k} y_j \log(\hat{y}_j)
$$

NÃªn ta cáº§n chuyá»ƒn nhÃ£n (0, 1, â€¦, kâˆ’1) â†’ vector one-hot.

###### 1.1 Code one-hot encoding:

```python
def convert_one_hot(y, k):  
    one_hot = np.zeros((len(y), k))
    one_hot[np.arange(len(y)), y] = 1
    return one_hot
```

###### 1.2 ThÃªm cá»™t Intercept

Softmax Regression dÃ¹ng:

$$
z = \theta^T x = 
\begin{bmatrix}
b \\
w
\end{bmatrix}^T
\begin{bmatrix}
1 \\
x
\end{bmatrix}
$$

NÃªn ta thÃªm 1 cá»™t toÃ n sá»‘ 1 vÃ o ma tráº­n `X`:

```python
  intercept = np.ones((X.shape[0], 1))
  X = np.concatenate((intercept, X), axis=1)
```

### B2: Khá»Ÿi táº¡o tham sá»‘ Î¸

Náº¿u sá»‘ chiá»u cá»§a input = 1 â†’ Î¸ cÃ³ shape (2 Ã— k).

VÃ­ dá»¥:

```python
theta = np.array([[0.1, 0.05], 
                  [0.2, -0.1]])
```

### B3: VÃ²ng láº·p huáº¥n luyá»‡n Softmax Regression

Huáº¥n luyá»‡n dá»±a theo cÃ´ng thá»©c gradient trong tÃ i liá»‡u:

**Forward:**

$$
z = \theta^T x
$$

$$
\hat{y} = \text{softmax}(z)
$$

**Loss (cross entropy):**

$$
L = -y^T \log(\hat{y})
$$

**Gradient:**

$$
\frac{\partial L}{\partial \theta} = x(\hat{y} - y)^T
$$

ğŸ”¥ **ToÃ n bá»™ vÃ²ng láº·p training:**

```python
learning_rate = 0.1
losses = []
max_epoch = 1

for epoch in range(max_epoch):
    for i in range(N): 
        xi = X[i]
        yi = y_one_hot[i]
        
        # reshape to column vectors
        xi = xi.reshape((2,1))
        yi = yi.reshape((2,1))
        
        # compute z
        z = theta.T.dot(xi)        
        
        # compute y_hat (softmax)
        exp_z = np.exp(z)
        y_hat = exp_z / np.sum(exp_z, axis=0)
        
        # compute loss
        loss = -yi.T.dot(np.log(y_hat))
        losses.append(loss[0])
        
        # compute gradient
        dz = y_hat - yi              # (2Ã—1)
        dtheta = xi.dot(dz.T)        # (2Ã—2)
        
        # update parameters
        theta = theta - learning_rate * dtheta
```


### Giáº£i thÃ­ch tá»«ng bÆ°á»›c

âœ” **Forward pass**  
Ta tÃ­nh $ z = \theta^T x $ vÃ  softmax $(\hat{y})$.

âœ” **Loss**  
Dá»±a Ä‘Ãºng cÃ´ng thá»©c trong tÃ i liá»‡u:

$$
L = -y^T \log(\hat{y})
$$

VÃ¬ \(y\) lÃ  one-hot â†’ chá»‰ láº¥y log(p) cá»§a class Ä‘Ãºng.

âœ” **Gradient**  
TÃ i liá»‡u chá»©ng minh:

$$
\frac{\partial L}{\partial z} = \hat{y} - y
$$

Tá»« chain rule:

$$
\frac{\partial L}{\partial \theta} = x (\hat{y} - y)^T
$$

âœ” **Update**  

$$
\theta := \theta - \eta \frac{\partial L}{\partial \theta}
$$